---
layout:     post
title:      "Personal View on Facebook's fastText"
subtitle:   "Thoughts on the heated fastText"
date:       2016-08-20
author:     "Chi Zhang"
header-img: "img/banner/post-banner-fastText.jpg" 
catalog: true
tags: 
    - Deep Learning
    - Natural Language Processing
---

> The [fastText paper](https://arxiv.org/abs/1607.01759) from Facebook AI Research (or **FAIR**) has been out for a while and all of a sudden a lot of people over the Internet surprise at how the FAIR team came up with such a fast and accurate algorithm, especially after several promotion-like blog posts, either by Facebook itself or some other tech-media. But is it really that significant and revolutionary? Before discussing my opinions on the paper, I think we should go over the paper first.

Wed, 6 July 2016, 19:40:15 GMT. arXiv witnessed the submission of a paper, ***Bag of Tricks for Efficient Text Classification***<sup>[[1]](#ref1)</sup>, co-authored by Tomas Mikolov. For those of you unfamiliar with him, Tomas proposed the famous *Word2Vec* algorithm and could be termed a pioneer on the distributed representation learning in the field of Natural Language Processing (readers interested in this would find a section devoted to it in my [previous blog post](http://wellyzhang.github.io/2016/05/09/demystify-deep-walk/)). And suddenly this paper incurred heated discussion on the Internet, even Yann Lecun tweeted about it, for the simplicity of the model architecture, the absolute accuracy, the performance and time trade-off and a big name in the author list. 

But I did read the paper and couldn't help asking myself: are ideas in the paper that powerful?

## Paper Overview

As the title of the paper indicates, authors explore the tricks in language models for the task of text classification. The model is pretty much similar to CBOW in [[2]](#ref2) as shown.

![architecture](/img/in-post/fastText/model.jpg)
<small class="img-hint">fastText model</small>

**It takes the word embeddings from a look-up table, averages them to obtain a vector representation of the sentence and applies *SoftMax* over it to get the prediction.** What only differs from CBOW is that in the output, this model predicts the label while CBOW predicts the middle word in the window. Pretty neat! 

Besides this model modification, authors also borrow the ideas of Hierarchical SoftMax and Negative Sampling from [[2]](#ref2) to speed up the computation.

Another trick that boosts the performance traces to additional N-gram features used in the prediction.

The authors also run several experiments to show the power of their model.

![result](/img/in-post/fastText/result.jpg)
<small class="img-hint">Experiments on sentiment analysis</small>

By the way, FAIR has open-sourced their fastText model in GitHub<sup>[[3]](#ref3)</sup>.

## Discussions

Well, I wouldn't deny it that FAIR has done a great job in text classification: the model is fast and nearly as accurate as deep neural network language models, the idea is neat and simple and does not fall in the "stereotype" of fancy RNNs or CNNs and it also somehow demonstrates that it's essential to understand and build the linear components in a language model while too complex ones that incorporate non-linearity like deep neural networks are not always necessary.

But there seems pretty much like ***HYPE*** over the Internet for the model right now.

#### Self-Promotion

Ever since the publication of the paper, Facebook has posted two same blogs for it, one on Facebook Research<sup>[[4]](#ref4)</sup> and another on Facebook Code<sup>[[5]](#ref5)</sup>, both of which give us a feel that Facebook has solved 

> a key part of the day-to-day interaction with your computer

and claims to advance the field of natural language processing. What's more, Yann Lecun has also joined in it, retweeting a blog that indicates Facebook fuses engineering and research, similar to one TechCrunch report. Some even say that Tomas Mikolov contributes to this paper so the quality is guaranteed. Oh, come on! Everybody in the academia knows that the first author tags his advisor's name just to make it appear good and attract more readers while indeed the advisor only gives few pieces of advice without contributing anything to coding or idea formulation. 

So it already seems like marketing.

#### Oversimplified Idea

However, the idea in the paper is a little bit oversimplified. Some even doubts that the model could be built with a one-line code change from Word2Vec and the idea is not worth so much attention because it is never comparable to Word2Vec in terms of contribution to the research community. Besides, it is easily observed in the table that BOW and ngrams models achieve similar performance to fastText and they are also clear and powerful. In that case, I could also one day build BOW and ngrams and claim they are orders of magnitude faster than and achieve comparable results to neural language models.

But is it meaningful?

#### Bias in Experiments

![bias](/img/in-post/fastText/bias.jpg)
<small class="img-hint">Bias in the experiment</small>

In the experiment, authors test fastText's performance on YFCC100M dataset. Note that in this dataset, every image caption is associated with multiple tags and Tagspace is designed to predict multiple tags, but authors just evaluate prec@1. Clearly, it is unfair to compare a model designed for multiple-tag prediction with another predicting exclusively a single tag.

So in conclusion, fastText does provide us some insights into the problem. It is interesting and impressive but there is seriously no need to make too much out of it. It's just another good paper.

## References

1. <a id="ref1">[Armand Joulin, Edouard Grave, Piotr Bojanowski and Tomas Mikolov. Bag of Tricks for Efficient Text Classification. *arXiv*: 1607.01759](https://arxiv.org/abs/1607.01759)</a>
2. <a id="ref2">[Tomas Mikolov, Kai Chen, Greg Corrado and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. *arXiv*: 1301.3781](https://arxiv.org/abs/1301.3781)</a>
3. <a id="ref3">[fastText model in GitHub by facebookresearch](https://github.com/facebookresearch/fastText)</a>
4. <a id="ref4">[fastText](https://research.facebook.com/blog/fasttext/)</a>
5. <a id="ref5">[FAIR open-sources fastText](https://code.facebook.com/posts/1438652669495149/fair-open-sources-fasttext)</a>

